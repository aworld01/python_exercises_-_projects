hi my name is vick and today we're going to build a speech recognition and summer is asian system and this system will be able to take any audio file a podcast the audio track of video lecture notes meeting recordings and turn them into a short summary the amazing thing about the system we're going to build is it's all gonna run on your local machine and it will be able to run using a c p you you're not going to need a fancy computer or a gp you to actually make this work will start out using a speech recognition library to transform audio recordings into taxed then we'll pass that into a summer's asian pipeline to actually generate a text summary of the audio file when you finish this project youth have a model the can actually recognized the text in an audio file using speech recognition and generate a transcript and then another model the can actually summarize that transcript to create a few paragraphs summary this is the summary of and about sixty minute long and pure marketplace podcast the transcription isn't perfect but you can see that the summary is pretty readable and it gives you a good just of the podcast marketplace marketplaces a podcast about current economic news so this is talking about you on musk trying to buy twitter you'll be able to use the same model on all sorts of audio files so whether they're lecture notes of recording of a meeting or anything else you'll be able to transcribe it and summarize it let's dive in and learn how to do it to actually do the speech recognition we're going to use a python library called bosc and vasquez support for over twenty languages although we're going to use english and waske is pretty easy to use in the models are pre trained so you can just download them and use them on your own machine so if we click over here on the left on models we can see all of the english models listed as we're going to be using this model which is fairly large but is fairly accurate and if you want a smaller download size you can also use one of the other models and i'll show you in a minute how you can actually download and use the models what we'll do now is we will jump into jupiter lab which is a great id highly recommend and will create a new notebook and will start coding there a couple of files that you're gonna need and i the link to these files is gonna be in the description of this video but there are two sample audio files marketplace full dog m p three and marketplace start m p three and we're going to use these to basically test our transcription model so if you have different audio files you can definitely use these use those but i highly recommend using these if you can download them so the first thing we're going to do is we're going to install bosc using pip install waske now we need to import so from vast we're going to import the model and a call d recognizer the model is actually going to load the pre trained model for us and the recognizer is going to use the model to actually recognize speech in audio files so let's go ahead and run those imports and then we're going to define a couple of constants so frame rate if you're familiar with audio this is actually just a sampling rate speech recognition works best with a sampling rate of sixteen hundred hertz don't worry too much if you don't know what this means this is just how high quality the audio as the higher the number the higher quality the audio is and then we need to specify chance so if you use headphones you may notice that sometimes the right headphone has an audio that's a little bit different from the left headphone like you may have drums that are little bit louder and one headphone versus another and what's happening there is you have to channel audio and those are essentially two different audio tracks that are both running and playing simultaneously and each year audio recognition works best with one audio channel so we're going to have to convert or audio to just be one channel and that's why we're defining this channels constant and then we can go ahead and load the model so what we're going to say is model equals model and then we have to specify a model me and as i mentioned i'm going to use this this model which is pretty large if you instantiate this it would make a one point eight gigabyte download but if you want to use a smaller model you should instead type of this and this models only fifty megabytes so it's a lot smaller than what we need to do is initialized are recognizer so we need to set up a called the recognizer which is what we imported we're going to pass in our model so that's the pre trained model with loaded and we're going to pass in or frame rate which is our sampling rate of our audio then what we're going to do is we're going to say wreck dots sacked words true we don't actually need to do this it's not one hundred percent necessary but this will show us both a complete tax transcript and individual words and the models confidence in those words so this helps us if if we want to correct a mistake or or anything else it gives us some information that we can use to do that so go ahead and run this and you'll see some warning date information here don't worry too much about this this is just call the loading the model and setting up the recognizer okay so i should have capitalized the us which is why i got an actual air at the end of all the warnings but now that i work okay the next thing we need to do is load an audio file that we can pass into our recognizer to actually recognize the text in the speech so to load the audio we're going to use a library called pie dog so much jump back over to chrome and i will show you pitre play dog is a python library that lets you do stuff to audio and if it isn't stupid is what the creator says but pie dog is basically a really nice way to work with audio it's a lot easier than some of the other libraries and python and you can load audio really easily and you can edit it really easily as well so we were are going to use that library to actually load our audio and then pass it into our speech recognition model okay let's go back to jupiter lab and code or so if you want to install pie dog it's just pip install i up pretty straightforward or run it i already installed so it'll say requirement already satisfied you may need to restart the colonel if you wanna restart your colonel you just click colonel at the top and then hit restart colonel i found it's not really necessary for these packages but if you want to be safe you definitely can die at the top of jupiter know poker jupiter lab if you're not using jupiter know poker jupiter lab the colonel restart maybe in a different place but i don't know what you're using so i i can advise you in that case right now we can do is we can import from pieta we're going to import audio segment and this is a class that enables us to load audio and manipulate it so i go ahead and run that and then we can load in our specific audi so i'm going to load in audio from a podcast called marketplace which is an economic podcast in the us and if you looked at the the read me that i sent out earlier it lists these files or marketplace and marketplace for and gives you download links to them towards the bottom but this marketplace or m p three is a forty five second segment from a marketplace part cats so this of this is a nice kind of baseline for our speech recognition so we're first going to load the fuck up then what we're gonna do is we're going to set the number of channels in our audio by default this audio is two channels but we're going to use pie dub to reduce it to one channel and then we're going to set the frame rate by default most podcasts i think around forty four hundred that at that's what their frame rate is forty four thousand but we're going to lower that to sixteen thousand so it's go ahead and run that okay great so what we've done now is we've initialized or model the can recognize speech and we've loaded in our speech recognition the the audio file the rapture going to do speech recognition on now all that's left to do is pass the audio file into the speech recognition model and to do that we call or the her when instantiated earlier this callie recognizer and we're going to call be except waveform method which is is a methodology pass in audio data and we're going to pass an m p three doc raw data so this is just actually i'll show you what it is and p three dark broad data this is a binary representation of the actual data in the m p three file so that's what we're going to pass into our recognizer and then what we're going to say is results equals rack dot results and this will give us the result of the speech recognition now we have a result so let's check what our result is so this is actually adjacent file i don't know why boss returns a son but it does so we actually have to load adjacent before he can use some going to import the jason library and i'm going to say text equals jason dutton loads results text and this will convert it into a python dictionary so it's check this out or and you can see this is now a text transcript of the forty five second audio clip and actually let's just take a look at what the for result looks like so you can see that because we passed this pure the set words true parameter we now have the individual confidences for each word and the timings so if what you wanted to do is is adjust the transcript you could look for words that the model had low confidence on and you could you could see if you needed to to changed it's okay to it's an excerpt are there is one big problem with this which you've probably have seen there's no punctuation it's all one long run on sense which is really really hard to read don't worry we will fix that shortly we will add punctuation and okay so we now have our transcript and it's not it's not punctuated it all so we need to fix that and that's the next thing will do right now we're going to use another library to actually add in the punctuation so i'll go back to the browser window and i'll show you what that library is so that library is called reek a spunk and what this does is you pass in text that doesn't have any punctuation and it adds the punctuation and it has a pre trained model that helps you do that and in fact actually waske has trained their own models using recast punk to convert vast output into output with punctuation so if we scroll down on this vast models list and we go down towards the bottom we can see these punctuation models so what we need to do is actually download this model and it's a big file so if this file is too large for you right now don't worry about downloading it will you can actually skip this piece and and things will still be okay and you can continue following along but if you do want to had punctuation you'll need to download this and this will download a zip file and then you'll need to unzip that file let me jump back to jupiter lap and i'll show you how i have unzipped it so i've unzipped it into this folder on the left to call to recast punk and if you downloaded unzip it you should see a few files here so you'll have this recast punk dot p y file you have an example and you should have this checkpoint fuck so this checkpoint files actually the pre trained model the will add the punctuation and you definitely need this file because this is the fire we were actually call to do our did you are in france so you'll need this checkpoint file and this file but all the zip file will have all of this stuff so the next thing we're going to do is go ahead and use that recast punk model to add punctuation and the easiest way to do this is actually to call the script that because already been written what we could do is try to write our own python code or import the script and run it but that is a little bit more complex and would require more code so what i'm going to do is i'm going to use some process and what's some process does is it basically uses python to run a terminal command so in this case the terminal command i'm running is calling the python interpreter to run another python five which is this replace punk dog p why file there was in that zip of the model and then we're going to say predicts and then we're going to pass in the checkpoint which is the pre trained model okay so what we could do is we could import this week is pumped up p y file as a python module like this like you could say import rico's punk the problem with that is you need to write a good amount of code to actually get it to work properly and instead of writing that code we can just call the script they've already created and then we're gonna need to pass in shell equals true which means run this in in the user shell we're going to pass in text equals true which means we're going to pass in some text input and then we're going to pass in our can put which is gonna be this this transcript that we just created for can and then was sign this to very vocal kissed okay yeah so if you're running if you're if you're running your jupiter notebook in a virtual environment then what happened is you installed these packages like pie dog into a virtual environment and if that's what happened then you need to use the same virtual environment to run this command you also need to install a couple of packages to get this to work so you'll need to install a package called transformers so pip install transformers which you can run you also need to install pie torch said torch and transformers is basically a package that will let you access to a lot of different retrained models it connects to hugging face hug which i'll show you a little bit of later there's a lot of cool pre trained models there and then torch is pipe watch so pie torch is a library created by facebook to that basically makes training deep learning models are a lot easier so this is the pie torch page and in order to run the recast punk model we need pie torch because it uses a torch code right strum back to coding are so you run pip install torch you don't actually need pip install rex so pip install torch and pip install transformers and then you can actually run this command change the order then you can run this command to to add casing to your text and then if you type in case you will get the transcript with for punctuation so you can see for example fed is capitalized fat is the federal reserve it is a proper noun you can see comments have been added and periods of been added to break up senses and actually looks pretty good this transcript is is pretty readable and it's it's fairly accurate which is grip the next piece is i'm going to write a function that can actually do all of this in one shot so you pass a file name into the function and it will do the it'll create the transcript and it'll go ahead at in the right casing so this function is going to be called voice recognition or so first we're going to define the model which we did earlier some going to copy and paste some of this code said this code specifically we first have our code to define our model then what we're going to do is again load or m p three for occur so a copy and paste that code down but instead of marketplace dot m p three we're going to load the file me so this this function will take a file name and then automatically do speech recognition on that file name now what if you pass in a file that longer than forty five seconds avast doesn't work really well on audio segments that are longer than like a minute or so memory usage goes up in france gets really slow so of we're going to do is actually batch longer audio files up into little pieces so let's create a variable boycott transcript and a variable called step and then what will say is for rain for i in range zero oh the length of the mp three file step so this step will basically do voice recognition on about forty five seconds of the audio file at a time and then we'll stitch it all back together so we're going to print say i divided by land and be three so this will just give us a progress percentage because that transcription can be really slow for long files then we'll take a segment of r m p three file so we'll see eye to eye plus step so this will take forty five seconds at a time the first time through the loop i will be zero and i plus step will be forty five thousand and and so on so it'll break the file into small segments and then we'll do what we did before will do rack dot except waveform which are go copy from above so we're going to do this instead of doing empty three though we're going to do segment the raw data will get the results and then we'll go ahead and actually load be taxed out of result and then we'll add that text to our transcript okay so this loop is as i mentioned it's just stepping through the file it's not exactly forty five seconds with forty five seconds or so at a time and then it is or transcribing that segment and adding it to the overall transcript and then when we're finished with that will go ahead and add in casing to our transcript we need to change the input to transcript and then we'll go ahead and return kissed so this function will transcribe an audio file add punctuation and return it i in range okay so let's try this let's do voice recognition marketplace dot m p three and see what happens here so first we are running loading and running of us model we actually probably don't need to reload it if it's already voted we could just pass it and that's an optimization we can make then it's going to do the actual recognition and then it's going to return the text so now it's doing the actual casing pieces piece of it and we should get the result back to perfect so this function this function can actually work with audio files have any length so you could pass in a longer file if i had time i would pass and marketplace full of that's going to take five minutes or so and you probably don't want to watch for to five minutes okay so we have our our function now that does a voice recognition the next thing i'm going to do is actually write the summer as a should coat so earlier we installed transformers with pip install transformers and we're actually going to use the same library to do our summers asian so let me jump back over to chrome and i'll show you plugging face so clogging face is a site that has lots of retrain models that have been up and and they also published the transformers library and through that library you can use a lot of these pre trained models so they have a pre trained model several praetorian models for summer as asian and we're going to use one of those models to actually summarize our text and if you ever want to explore hugging face there's tons of models for all sorts of different canopy tasks you can check out it's it's a great site and a great tool so let's go back over here and go ahead and do that so in order to do that first we're going to say from transformers import pipeline and then we're going to initialize or pipeline summarize are equals pipeline summers age and by default this will use a large a large model if you are which is about i think one and a half gigs to download if you want to use a smaller model just type in this specify that the model is t five small and i think that's around fifty megabytes so that should be a much faster download of course it's going to be less accurate but it it will be faster to download so let's go had an initial initialize that and then what we're gonna do is we need to split up our transcript into pieces so it's it's not a big concern here cause i didn't transcribe a long audio file but if you did you'd want to split your transcript up into smaller pieces because the summer as asian models in hugging face have a link for women and that's a fat one thousand twenty four tokens so a token could be a word could be a digit but you don't want to go over that that limit otherwise the model won't work so we're going to do is we're going to split are transcript on spaces this isn't a one hundred percent analogous to how hugging face token eyes as the input but it's somewhat close so what will say is len split tokens eight fifty so we're going to step through this transcript eight hundred and fifty not quite words but things that are separated with the space at a time and pass those into the hugging face summer as asian model so it will say selection equals split tokens eye to eye plus eight fifty okay so this will basically st same thing we did here we process to the audio file in batches were doing the same thing with our summers asian and were basically going to create a list i didn't create a transcript so actually transcribed a file earlier transcript that text this is in the get repo but this is a transcript of an entire marketplace episode so i'm gonna actually go ahead and load this f transcript because after okay some going to go hadn't read that and then what we can do is split it up so if we look at docs we see we now have a list of split up where the text has been split up it's not the smartest twist split up text ideally we would actually use the hugging face token either to token eyes the input and then split up based on something logical accents markers but this is a fast way to split and it works it works reasonably well for our purposes but there's a lot you can do to improve accuracy her and then what we do is we would say summaries cause summarize er docs and it would take this list of of taxed with about eight hundred fifty words in each in each piece of text and it's going to summarize each one this is going to take a little bit so now we have summaries so this is a list of dictionaries little bit hard to read so we're just gonna clean this up and we're going to say summary equals backslash and backslash n dot and join d summary text or d in summer it's so this is gonna loop through this is going to loop through this list and it's gonna grab the inside text from each dictionary and then it's going to concatenate all of them together so let's take a look at summary shoots print summary and we now have a summer and this is a summary of about a sixty minute podcast episode so we've really condensed down the summary isn't perfect because of how we token eyes and and kind of how we split the senses up but it's pretty good you can read it's pretty readable and you can tell what the podcast was about and i just love the sentence that it's in the summary my favorite and this episode if you're not familiar it was about you on mosques attempted takeover of twitter it also had some something about inflation in the us and and some of the other things that are going on economically in the us so yeah we now have a summary so we did a lot here we went from an audio file we built a recognizer that actually recognize that audio and converted it to text unfortunately that tax didn't have any punctuation so we added in the punctuation using another model and then we ended by actually building a function that can convert long audio files and once we had that function we were able to build a way to summarize what came out of it so what you could now do is build a system that can automatically summarize lecture notes recordings podcast whatever else you want to summarize one thing that i did want to talk about but i'm not enough time about to talk about is actually doing this live so creating a button you can hit to record audio and transcribe on the flight but hopefully our i'll have a video about doing that next week autumn can we install these local setups on visual studio code yes you can i'm using a jupiter notebook inside jupiter lab you can use jupiter notebooks in our vs could also can you talk about the trade off of the size of the model versus the accuracy yeah if you look at the vast page it actually lists the benchmark values for each model so you can actually see the trade off you'll be making it it has different datasets the benchmark each model on will this work in google cola yes the should work and google club although there may be memory limits the you'll hit their i'm not sure what the memory limit on columbus what sound type format does waske ingest it generally will ingest wave data so i'll show you a show you how to how to make that pile of the audio files never actually leave my local machine to that that's a great question the model is local all of this is local we're not using the cloud at all other than to download models but once they're downloaded were running them on our computer roger yes you can use larger audio files or show you a large audio file later you actually have to batshit and and run it in in batches so basically forty five seconds at a time is is what you have to do to actually do recognition across those files why do we reduce the channels to one so if you think about a speech recognition model it's a lot easier to train a model on one channel than two channels of audio because two channels is basically to separate audio files and really you just need one audio file to train speech recognition on it's not really to files it's two tracks but it's kind of two different tracks with slightly different timings and everything and it doesn't it may add a little bit of accuracy to model if you have two channels but it's a lot more complex to train the model some most pre trained audio recognition will be done on one channel audio and if the pre trained model was trained on one channel you obviously can't do in france with two channels so that that's why we're cutting it to out to lunch because the model was trained on one channel so we also after passing data that's only one channel to actually use it can i sat waske to use gp resources instead of cp you are yeah i'm using cp you hear because it's just a lot easier but you should be able to use gp you with waske i don't know if you get a big you you will get some speed up on in france and you can deftly check that in the waske documentation so if if you're not familiar with with that question cp use you can train deep learning models and do in france on cp use it's a lot slower than if you're using a gp you but not everyone has a gp you and configuring machine learning or deep learning frameworks to use a gp you can be really tough depending on what platform your on so it's usually easier to use the cp you if if you get if you can if performance isn't super important and if you're doing inference usually see you is is okay but if you're doing any training or fine tuning or anything you definitely should use a gp you are on what do you suggest beginners to start with to come to this love so i would recommend you have a good understanding of python and web a p eyes you actually don't need to understand the internals of machine learning and deep learning to do this because we're using pre trained models but it is it is very nice to actually be able to understand what's going on under the hood but don't let that stop you right you can call these libraries once you have a basic understanding of pandas and how to install packages and how to use a requests and call different web resources does waske support live stream connectors so no is the short answer you need to download audio and then transcribe it but what you can do and what are probably show in the next video is you can't record a segment and then transcribe it and those segments could be thirty seconds so you can kind of fake doing it live how would you host these so you wouldn't need to run a web server and you would need to essentially have a a backend that runs all this stuff that you're calling from a front end web service which is a little bit out of scope of of doing today but maybe i'll do in a future video robin vos has a lot of different models i don't know if they have in urdu model specific let me check they may by if not there's probably a pre trained or do model somewhere in some library you can use our you can train your own i would you can do with waske as well so it will work on different languages if you go to the vast models page which i'll share in chat there are several different languages they're listed there you can also train or fine tune a model self peter so yeah so two things like one learning how to do the stuff on your own is interesting and you can build projects the can go into your portfolio using this to google recognition tools and so google cloud or or kws if you're going to use those are actually pretty expensive and if you just want to build this for a hobby project and transcribe podcast it actually gets really expensive to do that so doing this on your own machine can be a lot cheaper and a lot more flexible depending on which language as you want to support etc i'm not familiar with the fast ai library so i don't know if they wrap boss or or anything else how can you train your own language model yeah so there are ways to train your own language model using va score other frameworks it's a little bit out of scope of what i'm going to talk about today but i may talk about that in a feature session can this functionality be wrapped in a tiny ai scope of device so vast does have model a small model that you can use on embedded devices like raspberry pi or something like that and the other models that it will show today some of the other ones do as well or right if not vos what library can do online transcription so i'm not sure if there are python library is to do online transcription i don't think you'll get great accuracy doing online transcription typically most speech recognition is done after the fact right like you say something and then the recognition happens if you wanted to fake my of transcription you could do it by transcribing thirty seconds at a time peter i'm not familiar with tiny a i i think you would need at least four gigs of ram to run this but i haven't tested that exhaustive the request punk script is from the zip file download so let's go back to chrome and let's go over here to the vast page so in vast models if you scroll down on this models page you'll see towards the bottom there are these punctuation models you want to download this and when you unzip this file the the case punk script will be in there on second to last line of transcript it says l h doing okay let's go back and take a look so i showed you before you can see the word confidence so if you go up if you look at g sun dot loads results that as and to do so if we load the result would chase on you can see the confidence is for each word so if you go down when you look through this you can start to see there are some words were confidence is pretty low and what you could do is go through and correct these manually or highlight them in some way when you display the transcript that's what these confidence is or vast i don't know boss works for multilingual data i assume you're talking about an audio file with multiple languages in it i think you need to train a custom model in that case what happens to the words at the cuts depends if there is a if they finished or not if they've been truncated then you will get poor recognition at the cuts so if you wanted to improve this what you would do is build a script actually search for silence in the in the audio file and caught at the silence football so what you need to do is extract the audio track from your video file and i don't know if you can do that with pied dog but there are python library is that what you do that can we use languages other than english yes waske actually has pre trained models for over twenty languages there are also other python speech recognition libraries that have other languages plowed yeah so there are ways to do this in the cloud if you want to pay like google cloud hosting this in the cloud could be complex but yeah that could be a interesting future video is there a word like t t y el de vos convert to talk to you later so it depends on how the model was trained so if we if we go up to waske so we we loaded a model hear a pre trained model called waske model the and us it depends how that model was train those models are typically trained by passing an audio files in transcripts of those files and those are used train the model so if those transcripts converted t t y our when you set it into talk to you later then the same thing would happen when we call the model but it just really depends how was trained thanks peter appreciate that why eight fifty again so the maximum token length for a for this summer asian pipeline the were using his one thousand twenty four and a token is approximately equivalent to a word but not one hundred percent equivalent to a word so we can take about one thousand twenty four words and each chunk that we pass into our summers asian pipeline now we're using a really naive way to split the transcript into words were just splitting based on a space so this split is different from what hugging faces doing so eight hundred fifty just gives us a little bit of a buffer between the one thousand twenty four limit and and how we're actually splitting ideally we would split the same way hugging face is and you can actually do that you can use the transformers library taxi token eyes your input by that's a lot more code and i wanted to show you something a little a little easier to work with can you turn text a voice wrestling you can do that there are lots of libraries to do that you can actually do that using hugging face transformers were using a different pipeline so if we go back to hugging face over here and fit the name always gets me hugging face and you click appear on models you can see models the do a lot of different tasks so you can do things like answering questions automatic speech recognition etc and i think you have text to speech yeah text to speech is a category here so there a lot of different models that can convert text to speech and if you click on one of these models it tells you how to use them so that can help you if you have a large yeah you absolutely can't with paul of course you will need to break the tax stop and batshit in order to pass it through the pipeline be i you can do that will this work for an audio file and other languages so yes let's go back to have lost over here and you can see these waske models so there are models in a lot of different languages including indian english so that's one a chinese russian french i think hindi is here to in the as well and if you if you can't find a model and vast that you want to use there are other frameworks other packages out there that can do voice recognition as well or you can train your own sonia yes you can fine tune the summers asian model from hugging face i don't know exactly the format of the data but if you go to hugging face you can look at the documentation on how to do that what's the easiest way to highlight the words with c l a less than one so i did this for the coven genome whereas analyzing covered genomic data but if you there's a certain way you can output data in here so you can basically use i python display functions the color certain characters or a certain certain letters or words or whatever so you'd want to use this this display function and this type of color print function to actually color the actual words and then you'd want to loop through that dictionary the came back from waske and identify any words with low confidence in actually color them so you could you could see them really clearly in the upper how would you add a feature to this which actually recognizes the person who is speaking yeah so these are called speaker identification models so the still example waske as a speaker identification model so that that is what you'd want to use their why waske so the reason i use of ask is because bosc is fairly high level so what what i showed you speech recognition you can do with a lot of different frameworks the nice thing about waske is it is really really did there's no lot of code you have to right okay so with have asked you can actually do all the speech recognition in very little code so you'll notice we we were basically able to create the model really easily and run it really easily other frameworks you have to write a lot more coat and for a alive tutorial we want to we want to write his little code as possible can i change from one language to another so typically most voice recognition models are trained only on a single language so if you if you are switching languages in your audio you would need to switch models is there a way to use this as an app you could you absolutely could create this as a web backend and then pass data to that back and like pass audio files to the back and running this at scale on a server is going to be a lot more complex than running it on your own machine but i might do a future video about that is there a model to cancel the noise yes there are the most popular one is called are and and noise it's by and video and it's used to filter out background noise but you can also just try and noise gate or like something simple like a noise floor to see if you're to see if you can cut out certain frequencies how many people can the speaker identification system support it really depends how many it's trained on i'm not sure about the vast one i haven't used it but are you could try it out and see how many speakers it can it can identify with how would you modify the model to recognize who was talking so you'd want to use a speaker identification model to do that which which you can there is a speaker identification model and waske i showed you there are also other speaker identification models journey absolutely you can do that so what you need to do is extract the text from the pdf there are python library that do that then you need to run tts on the extracted taxed so yeah you can you can absolutely do that typically a she's you're not gonna get a lot of benefit by using stereo audio for speech recognition because i mean one channel has most of the information that you're gonna get in in terms of speech recognition it might be useful on a speaker identification model to have to have two channels because you can tell which speakers which based on where the which channel the audio coming from son time you can find all of our previous webinars on you tube so me give you a link their yeah please subscribe to our channel was what getting more subscribers nocturnal i'm referring to the google and a double your speech recognition a pr it's so they do cost you money to use and if you're transcribing long audio like a podcast it's can get really expensive so i wouldn't recommend using them but they are pretty accurate and they obviously do everything for you are there any libraries the may be able to summarize the output justin so what you can do if you want to summarize the summary is you can are run the summary again on the summary to get to get even shorter a rights well thank you everyone have a great rest of your day wherever you are in the world have a good evening morning night